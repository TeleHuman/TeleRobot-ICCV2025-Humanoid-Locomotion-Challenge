Timer unit: 1e-09 s

Total time: 6.33375 s
File: /home/lxz/ICCV2025-Challenge/legged_gym/legged_gym/envs/base/humanoid_robot.py
Function: reset_idx at line 398

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   398                                               @profile
   399                                               def reset_idx(self, env_ids):
   400                                                   """ Reset some environments.
   401                                                       Calls self._reset_dofs(env_ids), self._reset_root_states(env_ids), and self._resample_commands(env_ids)
   402                                                       [Optional] calls self._update_terrain_curriculum(env_ids), self.update_command_curriculum(env_ids) and
   403                                                       Logs episode info
   404                                                       Resets some buffers
   405                                           
   406                                                   Args:
   407                                                       env_ids (list[int]): List of environment ids which must be reset
   408                                                   """
   409        26      76577.0   2945.3      0.0          import time
   410        26      31343.0   1205.5      0.0          start_time = time.time()
   411        26     132032.0   5078.2      0.0          if len(env_ids) == 0:
   412         3        450.0    150.0      0.0              return
   413                                                   
   414        23      16721.0    727.0      0.0          if self.save:
   415                                                       for env_id in env_ids:
   416                                                           try:
   417                                                               if len(self.current_episode_buffer['observations'][env_id]) > 750:
   418                                                                   # 转换为numpy数组
   419                                                                   episode_obs = np.stack(self.current_episode_buffer['observations'][env_id])  # [T,*]
   420                                                                   episode_act = np.stack(self.current_episode_buffer['actions'][env_id])       # [T,*]
   421                                                                   episode_rew = np.stack(self.current_episode_buffer['rewards'][env_id])      # [T]
   422                                                                   episode_hei = np.stack(self.current_episode_buffer['height_map'][env_id])      # [T, 396]
   423                                                                   episode_body = np.stack(self.current_episode_buffer['rigid_body_state'][env_id]) # [T,13,13] first is root
   424                                                                   episode_dof = np.stack(self.current_episode_buffer['dof_state'][env_id])
   425                                                                 
   426                                                                   # 存入主数据存储
   427                                                                   self.episode_data['observations'][env_id].append(episode_obs)
   428                                                                   self.episode_data['actions'][env_id].append(episode_act)
   429                                                                   self.episode_data['rewards'][env_id].append(episode_rew)
   430                                                                   self.episode_data['height_map'][env_id].append(episode_hei)
   431                                                                   self.episode_data['rigid_body_state'][env_id].append(episode_body)
   432                                                                   self.episode_data['dof_state'][env_id].append(episode_dof)
   433                                           
   434                                                                   
   435                                                                   # 处理privileged观测
   436                                                                   if self.privileged_obs_buf is not None:
   437                                                                       episode_priv = np.stack(self.current_episode_buffer['privileged_obs'][env_id]) # [T,*]
   438                                                                       self.episode_data['privileged_obs'][env_id].append(episode_priv)
   439                                                                   
   440                                                                   # 清空当前buffer
   441                                                                   self.current_episode_buffer['observations'][env_id] = []
   442                                                                   self.current_episode_buffer['actions'][env_id] = []
   443                                                                   self.current_episode_buffer['rewards'][env_id] = []
   444                                                                   self.current_episode_buffer['height_map'][env_id] = []
   445                                                                   self.current_episode_buffer['privileged_obs'][env_id] = []
   446                                                                   self.current_episode_buffer['rigid_body_state'][env_id] = []
   447                                                                   self.current_episode_buffer['dof_state'][env_id] = []
   448                                                                   
   449                                                                   print(f"Env {env_id} have saved {episode_obs.shape[0]} step data")
   450                                                           except Exception as e:
   451                                                               print(f"An error occured when saving env {env_id}: {str(e)}")
   452                                                   
   453                                                   # update curriculum
   454        23      17297.0    752.0      0.0          if self.cfg.terrain.curriculum:
   455        23   20438368.0 888624.7      0.3              self._update_terrain_curriculum(env_ids)
   456                                                   # avoid updating command curriculum at each step since the maximum command is common to all envs
   457        23      26134.0   1136.3      0.0          if self.cfg.commands.curriculum and (self.common_step_counter % self.max_episode_length==0):
   458                                                       self.update_command_curriculum(env_ids)
   459                                           
   460                                                   # reset robot states
   461        23   32320292.0 1.41e+06      0.5          self._reset_dofs(env_ids)
   462        23    9811689.0 426595.2      0.2          self._reset_root_states(env_ids)
   463        23    2718135.0 118179.8      0.0          self._resample_commands(env_ids)
   464        23 6094603248.0 2.65e+08     96.2          self.gym.simulate(self.sim)
   465        23   42704797.0 1.86e+06      0.7          self.gym.fetch_results(self.sim, True)
   466        23    5366472.0 233324.9      0.1          self.gym.refresh_rigid_body_state_tensor(self.sim)
   467                                           
   468                                                   # reset buffers
   469        23    5003427.0 217540.3      0.1          self.last_last_actions[env_ids] = 0.
   470        23    6047764.0 262946.3      0.1          self.last_actions[env_ids] = 0.
   471        23    7053273.0 306664.0      0.1          self.last_foot_action[env_ids] = 0.
   472        23    4958135.0 215571.1      0.1          self.last_dof_vel[env_ids] = 0.
   473        23    4559378.0 198233.8      0.1          self.last_torques[env_ids] = 0.
   474        23     634704.0  27595.8      0.0          self.last_root_vel[:] = 0.
   475        23    1938614.0  84287.6      0.0          self.feet_air_time[env_ids] = 0.
   476        23    3817783.0 165990.6      0.1          self.reset_buf[env_ids] = 1
   477        23    2265153.0  98484.9      0.0          self.obs_history_buf[env_ids, :, :] = 0.  # reset obs history buffer TODO no 0s
   478        23    2428933.0 105605.8      0.0          self.contact_buf[env_ids, :, :] = 0.
   479        23    4004200.0 174095.7      0.1          self.action_history_buf[env_ids, :, :] = 0.
   480        23    3185090.0 138482.2      0.1          self.cur_goal_idx[env_ids] = 0
   481        23    1999968.0  86955.1      0.0          self.reach_goal_timer[env_ids] = 0
   482                                           
   483        23    6065965.0 263737.6      0.1          self.phase_length_buf[env_ids] = 0 
   484                                           
   485                                                   # fill extras
   486        23      36741.0   1597.4      0.0          self.extras["episode"] = {}
   487       414     202989.0    490.3      0.0          for key in self.episode_sums.keys():
   488       391    8451717.0  21615.6      0.1              self.extras["episode"]['rew_' + key] = torch.mean(self.episode_sums[key][env_ids]) / self.max_episode_length_s
   489       391   59872776.0 153127.3      0.9              self.episode_sums[key][env_ids] = 0.
   490        23    2400393.0 104364.9      0.0          self.episode_length_buf[env_ids] = 0
   491                                           
   492                                                   # log additional curriculum info
   493        23      54472.0   2368.3      0.0          if self.cfg.terrain.curriculum:
   494                                                       # print("terrain level = ", self.terrain_levels, " mean  ", torch.mean(self.terrain_levels.float()))
   495        23     357333.0  15536.2      0.0              self.extras["episode"]["terrain_level"] = torch.mean(self.terrain_levels.float())
   496        23      27211.0   1183.1      0.0          if self.cfg.commands.curriculum:
   497                                                       self.extras["episode"]["max_command_x"] = self.command_ranges["lin_vel_x"][1]
   498                                                   # send timeout info to the algorithm
   499        23      32647.0   1419.4      0.0          if self.cfg.env.send_timeouts:
   500        23      37302.0   1621.8      0.0              self.extras["time_outs"] = self.time_out_buf
   501                                           
   502        23      49321.0   2144.4      0.0          end_time = time.time()
   503                                                   # print(f"Reset {len(env_ids)} envs, took {end_time - start_time:.3f} seconds")

Total time: 6.60856 s
File: /home/lxz/ICCV2025-Challenge/legged_gym/legged_gym/envs/base/humanoid_robot.py
Function: post_physics_step at line 299

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   299                                               @profile
   300                                               def post_physics_step(self):
   301                                                   """ check terminations, compute observations and rewards
   302                                                       calls self._post_physics_step_callback() for common computations 
   303                                                       calls self._draw_debug_vis() if needed
   304                                                   """
   305        25    7092847.0 283713.9      0.1          self.gym.refresh_actor_root_state_tensor(self.sim)
   306        25     327954.0  13118.2      0.0          self.gym.refresh_net_contact_force_tensor(self.sim)
   307        25    5356089.0 214243.6      0.1          self.gym.refresh_rigid_body_state_tensor(self.sim)
   308                                                   # self.gym.refresh_force_sensor_tensor(self.sim)
   309                                           
   310        25    1895790.0  75831.6      0.0          self.episode_length_buf += 1
   311        25     207667.0   8306.7      0.0          self.phase_length_buf += 1 
   312        25      22374.0    895.0      0.0          self.common_step_counter += 1
   313                                           
   314                                                   # prepare quantities
   315        25    1083544.0  43341.8      0.0          self.base_quat[:] = self.root_states[:, 3:7]
   316        25    5110689.0 204427.6      0.1          self.base_lin_vel[:] = quat_rotate_inverse(self.base_quat, self.root_states[:, 7:10])
   317        25    1525068.0  61002.7      0.0          self.base_ang_vel[:] = quat_rotate_inverse(self.base_quat, self.root_states[:, 10:13])
   318        25    1300342.0  52013.7      0.0          self.projected_gravity[:] = quat_rotate_inverse(self.base_quat, self.gravity_vec)
   319        25    1127895.0  45115.8      0.0          self.base_lin_acc = (self.root_states[:, 7:10] - self.last_root_vel[:, :3]) / self.dt
   320                                           
   321        25    1114530.0  44581.2      0.0          self.knee_pos = self.rigid_body_states.view(self.num_envs, self.num_bodies, 13)[:, self.knee_indices, 0:3]
   322        25     426255.0  17050.2      0.0          self.feet_pos = self.rigid_body_states.view(self.num_envs, self.num_bodies, 13)[:, self.feet_indices, 0:3]
   323                                           
   324        25    9462483.0 378499.3      0.1          self.roll, self.pitch, self.yaw = euler_from_quaternion(self.base_quat)
   325                                           
   326        25    2040288.0  81611.5      0.0          contact = torch.norm(self.contact_forces[:, self.feet_indices], dim=-1) > 2.
   327        25    7827926.0 313117.0      0.1          self.contact_filt = torch.logical_or(contact, self.last_contacts) 
   328        25      50656.0   2026.2      0.0          self.last_contacts = contact
   329                                                   
   330                                                   # self._update_jump_schedule()
   331        25   70065639.0  2.8e+06      1.1          self._update_goals()
   332        25   98948808.0 3.96e+06      1.5          self._post_physics_step_callback()
   333                                           
   334                                                   # compute observations, rewards, resets, ...
   335        25   34122201.0 1.36e+06      0.5          self.check_termination()
   336        25   55402354.0 2.22e+06      0.8          self.compute_reward()
   337        25   10461387.0 418455.5      0.2          env_ids = self.reset_buf.nonzero(as_tuple=False).flatten()
   338        25 6275079956.0 2.51e+08     95.0          self.reset_idx(env_ids)
   339                                           
   340        25    1297453.0  51898.1      0.0          self.cur_goals = self._gather_cur_goals()
   341        25     340773.0  13630.9      0.0          self.next_goals = self._gather_cur_goals(future=1)
   342                                           
   343                                                   # self.update_depth_buffer()
   344                                           
   345        25   10429730.0 417189.2      0.2          self.compute_observations() # in some cases a simulation step might be required to refresh some obs (for example body positions)
   346                                           
   347        25     191853.0   7674.1      0.0          self.last_last_actions[:] = self.last_actions[:]
   348        25     116695.0   4667.8      0.0          self.last_actions[:] = self.actions[:]
   349        25     152641.0   6105.6      0.0          self.last_dof_vel[:] = self.dof_vel[:]
   350        25     126308.0   5052.3      0.0          self.last_torques[:] = self.torques[:]
   351        25     139703.0   5588.1      0.0          self.last_root_vel[:] = self.root_states[:, 7:13]
   352        25      18507.0    740.3      0.0          if(self.time_stamp ==5):
   353         4      42646.0  10661.5      0.0              self.last_foot_action = self.rigid_body_states[:, self.feet_indices, :]
   354         4       1221.0    305.2      0.0              self.time_stamp=0
   355                                                   else :
   356        21       7661.0    364.8      0.0              self.time_stamp=self.time_stamp+1
   357                                                   
   358        25      10052.0    402.1      0.0          if self.viewer and self.enable_viewer_sync and self.debug_viz:
   359                                                       self.gym.clear_lines(self.viewer)
   360                                                       # self._draw_height_samples()
   361                                                       self._draw_goals()
   362                                                       # self._draw_feet()
   363                                                       if self.cfg.depth.use_camera:
   364                                                           window_name = "Depth Image"
   365                                                           cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
   366                                                           cv2.imshow("Depth Image", self.depth_buffer[self.lookat_id, -1].cpu().numpy() + 0.5)
   367                                                           cv2.waitKey(1)
   368                                           
   369        25     179633.0   7185.3      0.0          cur_knee_pos_trans = self.knee_pos - self.root_states[:, 0:3].unsqueeze(1)
   370        75     328823.0   4384.3      0.0          for i in range(len(self.knee_indices)):
   371        50    3526598.0  70532.0      0.1              self.knee_pos_in_body[:, i, :] = quat_rotate_inverse(self.base_quat, cur_knee_pos_trans[:, i, :])
   372                                           
   373        25     180602.0   7224.1      0.0          cur_feet_pos_trans = self.feet_pos - self.root_states[:, 0:3].unsqueeze(1)
   374        75      94335.0   1257.8      0.0          for i in range(len(self.feet_indices)):
   375        50    1322882.0  26457.6      0.0              self.feet_pos_in_body[:, i, :] = quat_rotate_inverse(self.base_quat, cur_feet_pos_trans[:, i, :])

Total time: 24.0284 s
File: /home/lxz/ICCV2025-Challenge/legged_gym/legged_gym/envs/base/humanoid_robot.py
Function: step at line 151

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   151                                               @profile
   152                                               def step(self, actions):
   153                                                   """ Apply actions, simulate, call self.post_physics_step()
   154                                           
   155                                                   Args:
   156                                                       actions (torch.Tensor): Tensor of shape (num_envs, num_actions_per_env)
   157                                                   """
   158        24      23898.0    995.8      0.0          start_t = time()
   159        24      93767.0   3907.0      0.0          actions.to(self.device)
   160        24     827488.0  34478.7      0.0          self.action_history_buf = torch.cat([self.action_history_buf[:, 1:].clone(), actions[:, None, :].clone()], dim=1)
   161        24      29982.0   1249.2      0.0          if self.cfg.domain_rand.action_delay:
   162        24      32048.0   1335.3      0.0              if self.global_counter % self.cfg.domain_rand.delay_update_global_steps == 0:
   163         1       1362.0   1362.0      0.0                  if len(self.cfg.domain_rand.action_curr_step) != 0:
   164         1     871575.0 871575.0      0.0                      self.delay = torch.tensor(self.cfg.domain_rand.action_curr_step.pop(0), device=self.device, dtype=torch.float)
   165        24      18909.0    787.9      0.0              if self.viewer:
   166                                                           self.delay = torch.tensor(self.cfg.domain_rand.action_delay_view, device=self.device, dtype=torch.float)
   167        24     194154.0   8089.8      0.0              indices = -self.delay -1
   168        24    4495071.0 187294.6      0.0              actions = self.action_history_buf[:, indices.long()] # delay for 1/50=20ms
   169                                           
   170        24      16382.0    682.6      0.0          self.global_counter += 1
   171        24      13429.0    559.5      0.0          self.total_env_steps_counter += 1
   172        24      40577.0   1690.7      0.0          clip_actions = self.cfg.normalization.clip_actions / self.cfg.control.action_scale
   173        24     277014.0  11542.2      0.0          self.actions = torch.clip(actions, -clip_actions, clip_actions).to(self.device)
   174        24      59053.0   2460.5      0.0          self.render()
   175                                                   # print("1111 coumse: ", time()-start_t)
   176                                           
   177        24      12514.0    521.4      0.0          start_t = time()
   178       120     136634.0   1138.6      0.0          for _ in range(self.cfg.control.decimation):
   179        96   14454156.0 150564.1      0.1              self.torques = self._compute_torques(self.actions).view(self.torques.shape)
   180        96   79009065.0 823011.1      0.3              self.gym.set_dof_actuation_force_tensor(self.sim, gymtorch.unwrap_tensor(self.torques))
   181        96     1.73e+10  1.8e+08     71.8              self.gym.simulate(self.sim)
   182        96  175384685.0 1.83e+06      0.7              self.gym.fetch_results(self.sim, True)
   183        96   26892361.0 280128.8      0.1              self.gym.refresh_dof_state_tensor(self.sim)
   184        24 6465180914.0 2.69e+08     26.9          self.post_physics_step()
   185                                                   # print("2222 coumse: ", time()-start_t)
   186                                           
   187        24      18156.0    756.5      0.0          start_t = time()
   188        24      29921.0   1246.7      0.0          clip_obs = self.cfg.normalization.clip_observations
   189        24     154347.0   6431.1      0.0          self.obs_buf = torch.clip(self.obs_buf, -clip_obs, clip_obs)
   190        24       7846.0    326.9      0.0          if self.privileged_obs_buf is not None:
   191        24      98802.0   4116.8      0.0              self.privileged_obs_buf = torch.clip(self.privileged_obs_buf, -clip_obs, clip_obs)
   192        24     247346.0  10306.1      0.0          self.extras["delta_yaw_ok"] = self.delta_yaw < 0.6
   193        24      32582.0   1357.6      0.0          if self.cfg.depth.use_camera and self.global_counter % self.cfg.depth.update_interval == 0:
   194                                                       self.extras["depth"] = self.depth_buffer[:, -2]  # have already selected last one
   195                                                   else:
   196        24       8187.0    341.1      0.0              self.extras["depth"] = None
   197                                                   # print("3333 coumse: ", time()-start_t)
   198                                           
   199        24      12040.0    501.7      0.0          if self.save:
   200                                                       for env_idx in range(self.num_envs):
   201                                                           self.current_episode_buffer['observations'][env_idx].append(
   202                                                               self.obs_buf[env_idx].cpu().numpy().copy())  
   203                                                           self.current_episode_buffer['actions'][env_idx].append(
   204                                                               self.actions[env_idx].cpu().numpy().copy())      
   205                                                           
   206                                                           self.current_episode_buffer['rewards'][env_idx].append(
   207                                                               self.rew_buf[env_idx].cpu().numpy().copy()) 
   208                                                           
   209                                                           self.current_episode_buffer['height_map'][env_idx].append(
   210                                                               self.measured_heights_data[env_idx].cpu().numpy().copy()) 
   211                                                           
   212                                                           self.current_episode_buffer['rigid_body_state'][env_idx].append(
   213                                                               self.rigid_body_states[env_idx].cpu().numpy().copy()) 
   214                                                           
   215                                                           self.current_episode_buffer['dof_state'][env_idx].append(
   216                                                               self.dof_state[env_idx].cpu().numpy().copy())  
   217                                           
   218                                                           if self.privileged_obs_buf is not None:
   219                                                               self.current_episode_buffer['privileged_obs'][env_idx].append(
   220                                                                   self.privileged_obs_buf[env_idx].cpu().numpy().copy())      
   221                                           
   222        24      23279.0    970.0      0.0          if(self.cfg.rewards.is_play):
   223                                                       if(self.total_times > 0):
   224                                                           if(self.total_times > self.last_times):
   225                                                               # print("total_times=",self.total_times)
   226                                                               # print("success_rate=",self.success_times / self.total_times)
   227                                                               # print("complete_rate=",(self.complete_times / self.total_times).cpu().numpy().copy())
   228                                                               self.last_times = self.total_times
   229                                           
   230        24      11160.0    465.0      0.0          return self.obs_buf, self.privileged_obs_buf, self.rew_buf, self.reset_buf, self.extras

  6.33 seconds - /home/lxz/ICCV2025-Challenge/legged_gym/legged_gym/envs/base/humanoid_robot.py:398 - reset_idx
  6.61 seconds - /home/lxz/ICCV2025-Challenge/legged_gym/legged_gym/envs/base/humanoid_robot.py:299 - post_physics_step
 24.03 seconds - /home/lxz/ICCV2025-Challenge/legged_gym/legged_gym/envs/base/humanoid_robot.py:151 - step
