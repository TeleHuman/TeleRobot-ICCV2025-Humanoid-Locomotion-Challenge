Timer unit: 1e-09 s

Total time: 823.066 s
File: /home/lxz/ICCV2025-Challenge/legged_gym/legged_gym/envs/base/humanoid_robot.py
Function: step at line 151

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   151                                               @profile
   152                                               def step(self, actions):
   153                                                   """ Apply actions, simulate, call self.post_physics_step()
   154                                           
   155                                                   Args:
   156                                                       actions (torch.Tensor): Tensor of shape (num_envs, num_actions_per_env)
   157                                                   """
   158       733     729247.0    994.9      0.0          start_t = time()
   159       733    3016970.0   4115.9      0.0          actions.to(self.device)
   160       733   25656193.0  35001.6      0.0          self.action_history_buf = torch.cat([self.action_history_buf[:, 1:].clone(), actions[:, None, :].clone()], dim=1)
   161       733    1075915.0   1467.8      0.0          if self.cfg.domain_rand.action_delay:
   162       733    1093346.0   1491.6      0.0              if self.global_counter % self.cfg.domain_rand.delay_update_global_steps == 0:
   163         1       1012.0   1012.0      0.0                  if len(self.cfg.domain_rand.action_curr_step) != 0:
   164         1     158798.0 158798.0      0.0                      self.delay = torch.tensor(self.cfg.domain_rand.action_curr_step.pop(0), device=self.device, dtype=torch.float)
   165       733     283842.0    387.2      0.0              if self.viewer:
   166                                                           self.delay = torch.tensor(self.cfg.domain_rand.action_delay_view, device=self.device, dtype=torch.float)
   167       733    6262897.0   8544.2      0.0              indices = -self.delay -1
   168       733  215497155.0 293993.4      0.0              actions = self.action_history_buf[:, indices.long()] # delay for 1/50=20ms
   169                                           
   170       733     458946.0    626.1      0.0          self.global_counter += 1
   171       733     636244.0    868.0      0.0          self.total_env_steps_counter += 1
   172       733    1112158.0   1517.3      0.0          clip_actions = self.cfg.normalization.clip_actions / self.cfg.control.action_scale
   173       733    8668677.0  11826.3      0.0          self.actions = torch.clip(actions, -clip_actions, clip_actions).to(self.device)
   174       733    1724137.0   2352.2      0.0          self.render()
   175                                                   # print("1111 coumse: ", time()-start_t)
   176                                           
   177       733     296592.0    404.6      0.0          start_t = time()
   178      3665    3791404.0   1034.5      0.0          for _ in range(self.cfg.control.decimation):
   179      2932  443661818.0 151317.1      0.1              self.torques = self._compute_torques(self.actions).view(self.torques.shape)
   180      2932 1861475789.0 634882.6      0.2              self.gym.set_dof_actuation_force_tensor(self.sim, gymtorch.unwrap_tensor(self.torques))
   181      2932     5.34e+11 1.82e+08     64.9              self.gym.simulate(self.sim)
   182      2932 5551198231.0 1.89e+06      0.7              self.gym.fetch_results(self.sim, True)
   183      2932  820605824.0 279879.2      0.1              self.gym.refresh_dof_state_tensor(self.sim)
   184       733      2.8e+11 3.82e+08     34.0          self.post_physics_step()
   185                                                   # print("2222 coumse: ", time()-start_t)
   186                                           
   187       732     895807.0   1223.8      0.0          start_t = time()
   188       732     771636.0   1054.1      0.0          clip_obs = self.cfg.normalization.clip_observations
   189       732    5620033.0   7677.6      0.0          self.obs_buf = torch.clip(self.obs_buf, -clip_obs, clip_obs)
   190       732     255489.0    349.0      0.0          if self.privileged_obs_buf is not None:
   191       732    3015624.0   4119.7      0.0              self.privileged_obs_buf = torch.clip(self.privileged_obs_buf, -clip_obs, clip_obs)
   192       732    7793766.0  10647.2      0.0          self.extras["delta_yaw_ok"] = self.delta_yaw < 0.6
   193       732     782235.0   1068.6      0.0          if self.cfg.depth.use_camera and self.global_counter % self.cfg.depth.update_interval == 0:
   194                                                       self.extras["depth"] = self.depth_buffer[:, -2]  # have already selected last one
   195                                                   else:
   196       732     184419.0    251.9      0.0              self.extras["depth"] = None
   197                                                   # print("3333 coumse: ", time()-start_t)
   198                                           
   199       732     395392.0    540.2      0.0          if self.save:
   200                                                       for env_idx in range(self.num_envs):
   201                                                           self.current_episode_buffer['observations'][env_idx].append(
   202                                                               self.obs_buf[env_idx].cpu().numpy().copy())  
   203                                                           self.current_episode_buffer['actions'][env_idx].append(
   204                                                               self.actions[env_idx].cpu().numpy().copy())      
   205                                                           
   206                                                           self.current_episode_buffer['rewards'][env_idx].append(
   207                                                               self.rew_buf[env_idx].cpu().numpy().copy()) 
   208                                                           
   209                                                           self.current_episode_buffer['height_map'][env_idx].append(
   210                                                               self.measured_heights_data[env_idx].cpu().numpy().copy()) 
   211                                                           
   212                                                           self.current_episode_buffer['rigid_body_state'][env_idx].append(
   213                                                               self.rigid_body_states[env_idx].cpu().numpy().copy()) 
   214                                                           
   215                                                           self.current_episode_buffer['dof_state'][env_idx].append(
   216                                                               self.dof_state[env_idx].cpu().numpy().copy())  
   217                                           
   218                                                           if self.privileged_obs_buf is not None:
   219                                                               self.current_episode_buffer['privileged_obs'][env_idx].append(
   220                                                                   self.privileged_obs_buf[env_idx].cpu().numpy().copy())      
   221                                           
   222       732     689403.0    941.8      0.0          if(self.cfg.rewards.is_play):
   223                                                       if(self.total_times > 0):
   224                                                           if(self.total_times > self.last_times):
   225                                                               # print("total_times=",self.total_times)
   226                                                               # print("success_rate=",self.success_times / self.total_times)
   227                                                               # print("complete_rate=",(self.complete_times / self.total_times).cpu().numpy().copy())
   228                                                               self.last_times = self.total_times
   229                                           
   230       732     376550.0    514.4      0.0          return self.obs_buf, self.privileged_obs_buf, self.rew_buf, self.reset_buf, self.extras

823.07 seconds - /home/lxz/ICCV2025-Challenge/legged_gym/legged_gym/envs/base/humanoid_robot.py:151 - step
