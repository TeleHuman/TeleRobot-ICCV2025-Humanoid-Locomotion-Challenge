Timer unit: 1e-09 s

Total time: 6.41991 s
File: /home/lxz/ICCV2025-Challenge/legged_gym/legged_gym/envs/base/humanoid_robot.py
Function: reset_idx at line 398

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   398                                               @profile
   399                                               def reset_idx(self, env_ids):
   400                                                   """ Reset some environments.
   401                                                       Calls self._reset_dofs(env_ids), self._reset_root_states(env_ids), and self._resample_commands(env_ids)
   402                                                       [Optional] calls self._update_terrain_curriculum(env_ids), self.update_command_curriculum(env_ids) and
   403                                                       Logs episode info
   404                                                       Resets some buffers
   405                                           
   406                                                   Args:
   407                                                       env_ids (list[int]): List of environment ids which must be reset
   408                                                   """
   409        26      74456.0   2863.7      0.0          import time
   410        26      28723.0   1104.7      0.0          start_time = time.time()
   411        26     130985.0   5037.9      0.0          if len(env_ids) == 0:
   412         3        524.0    174.7      0.0              return
   413                                                   
   414        23       9523.0    414.0      0.0          if self.save:
   415                                                       for env_id in env_ids:
   416                                                           try:
   417                                                               if len(self.current_episode_buffer['observations'][env_id]) > 750:
   418                                                                   # 转换为numpy数组
   419                                                                   episode_obs = np.stack(self.current_episode_buffer['observations'][env_id])  # [T,*]
   420                                                                   episode_act = np.stack(self.current_episode_buffer['actions'][env_id])       # [T,*]
   421                                                                   episode_rew = np.stack(self.current_episode_buffer['rewards'][env_id])      # [T]
   422                                                                   episode_hei = np.stack(self.current_episode_buffer['height_map'][env_id])      # [T, 396]
   423                                                                   episode_body = np.stack(self.current_episode_buffer['rigid_body_state'][env_id]) # [T,13,13] first is root
   424                                                                   episode_dof = np.stack(self.current_episode_buffer['dof_state'][env_id])
   425                                                                 
   426                                                                   # 存入主数据存储
   427                                                                   self.episode_data['observations'][env_id].append(episode_obs)
   428                                                                   self.episode_data['actions'][env_id].append(episode_act)
   429                                                                   self.episode_data['rewards'][env_id].append(episode_rew)
   430                                                                   self.episode_data['height_map'][env_id].append(episode_hei)
   431                                                                   self.episode_data['rigid_body_state'][env_id].append(episode_body)
   432                                                                   self.episode_data['dof_state'][env_id].append(episode_dof)
   433                                           
   434                                                                   
   435                                                                   # 处理privileged观测
   436                                                                   if self.privileged_obs_buf is not None:
   437                                                                       episode_priv = np.stack(self.current_episode_buffer['privileged_obs'][env_id]) # [T,*]
   438                                                                       self.episode_data['privileged_obs'][env_id].append(episode_priv)
   439                                                                   
   440                                                                   # 清空当前buffer
   441                                                                   self.current_episode_buffer['observations'][env_id] = []
   442                                                                   self.current_episode_buffer['actions'][env_id] = []
   443                                                                   self.current_episode_buffer['rewards'][env_id] = []
   444                                                                   self.current_episode_buffer['height_map'][env_id] = []
   445                                                                   self.current_episode_buffer['privileged_obs'][env_id] = []
   446                                                                   self.current_episode_buffer['rigid_body_state'][env_id] = []
   447                                                                   self.current_episode_buffer['dof_state'][env_id] = []
   448                                                                   
   449                                                                   print(f"Env {env_id} have saved {episode_obs.shape[0]} step data")
   450                                                           except Exception as e:
   451                                                               print(f"An error occured when saving env {env_id}: {str(e)}")
   452                                                   
   453                                                   # update curriculum
   454        23      30896.0   1343.3      0.0          if self.cfg.terrain.curriculum:
   455        23   19285664.0 838507.1      0.3              self._update_terrain_curriculum(env_ids)
   456                                                   # avoid updating command curriculum at each step since the maximum command is common to all envs
   457        23      28324.0   1231.5      0.0          if self.cfg.commands.curriculum and (self.common_step_counter % self.max_episode_length==0):
   458                                                       self.update_command_curriculum(env_ids)
   459                                           
   460                                                   # reset robot states
   461        23   16524000.0 718434.8      0.3          self._reset_dofs(env_ids)
   462        23    9331069.0 405698.7      0.1          self._reset_root_states(env_ids)
   463        23    2491799.0 108339.1      0.0          self._resample_commands(env_ids)
   464        23 6186678158.0 2.69e+08     96.4          self.gym.simulate(self.sim)
   465        23   62566092.0 2.72e+06      1.0          self.gym.fetch_results(self.sim, True)
   466        23    9583445.0 416671.5      0.1          self.gym.refresh_rigid_body_state_tensor(self.sim)
   467                                           
   468                                                   # reset buffers
   469        23    8492626.0 369244.6      0.1          self.last_last_actions[env_ids] = 0.
   470        23    7352968.0 319694.3      0.1          self.last_actions[env_ids] = 0.
   471        23    5247837.0 228166.8      0.1          self.last_foot_action[env_ids] = 0.
   472        23    3043727.0 132336.0      0.0          self.last_dof_vel[env_ids] = 0.
   473        23    2133814.0  92774.5      0.0          self.last_torques[env_ids] = 0.
   474        23     671913.0  29213.6      0.0          self.last_root_vel[:] = 0.
   475        23    1977292.0  85969.2      0.0          self.feet_air_time[env_ids] = 0.
   476        23    4440098.0 193047.7      0.1          self.reset_buf[env_ids] = 1
   477        23    4255449.0 185019.5      0.1          self.obs_history_buf[env_ids, :, :] = 0.  # reset obs history buffer TODO no 0s
   478        23    2453396.0 106669.4      0.0          self.contact_buf[env_ids, :, :] = 0.
   479        23    2924875.0 127168.5      0.0          self.action_history_buf[env_ids, :, :] = 0.
   480        23    1927857.0  83819.9      0.0          self.cur_goal_idx[env_ids] = 0
   481        23    3594774.0 156294.5      0.1          self.reach_goal_timer[env_ids] = 0
   482                                           
   483        23    1794150.0  78006.5      0.0          self.phase_length_buf[env_ids] = 0 
   484                                           
   485                                                   # fill extras
   486        23      41497.0   1804.2      0.0          self.extras["episode"] = {}
   487       414     203010.0    490.4      0.0          for key in self.episode_sums.keys():
   488       391   11413993.0  29191.8      0.2              self.extras["episode"]['rew_' + key] = torch.mean(self.episode_sums[key][env_ids]) / self.max_episode_length_s
   489       391   48542436.0 124149.5      0.8              self.episode_sums[key][env_ids] = 0.
   490        23    2070940.0  90040.9      0.0          self.episode_length_buf[env_ids] = 0
   491                                           
   492                                                   # log additional curriculum info
   493        23      63737.0   2771.2      0.0          if self.cfg.terrain.curriculum:
   494                                                       # print("terrain level = ", self.terrain_levels, " mean  ", torch.mean(self.terrain_levels.float()))
   495        23     356168.0  15485.6      0.0              self.extras["episode"]["terrain_level"] = torch.mean(self.terrain_levels.float())
   496        23      36747.0   1597.7      0.0          if self.cfg.commands.curriculum:
   497                                                       self.extras["episode"]["max_command_x"] = self.command_ranges["lin_vel_x"][1]
   498                                                   # send timeout info to the algorithm
   499        23      29612.0   1287.5      0.0          if self.cfg.env.send_timeouts:
   500        23      32741.0   1423.5      0.0              self.extras["time_outs"] = self.time_out_buf
   501                                           
   502        23      49539.0   2153.9      0.0          end_time = time.time()
   503                                                   # print(f"Reset {len(env_ids)} envs, took {end_time - start_time:.3f} seconds")

Total time: 6.73947 s
File: /home/lxz/ICCV2025-Challenge/legged_gym/legged_gym/envs/base/humanoid_robot.py
Function: post_physics_step at line 299

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   299                                               @profile
   300                                               def post_physics_step(self):
   301                                                   """ check terminations, compute observations and rewards
   302                                                       calls self._post_physics_step_callback() for common computations 
   303                                                       calls self._draw_debug_vis() if needed
   304                                                   """
   305        25    6952957.0 278118.3      0.1          self.gym.refresh_actor_root_state_tensor(self.sim)
   306        25     300604.0  12024.2      0.0          self.gym.refresh_net_contact_force_tensor(self.sim)
   307        25    7974623.0 318984.9      0.1          self.gym.refresh_rigid_body_state_tensor(self.sim)
   308                                                   # self.gym.refresh_force_sensor_tensor(self.sim)
   309                                           
   310        25    1880065.0  75202.6      0.0          self.episode_length_buf += 1
   311        25     185926.0   7437.0      0.0          self.phase_length_buf += 1 
   312        25      30385.0   1215.4      0.0          self.common_step_counter += 1
   313                                           
   314                                                   # prepare quantities
   315        25    1038201.0  41528.0      0.0          self.base_quat[:] = self.root_states[:, 3:7]
   316        25    4804157.0 192166.3      0.1          self.base_lin_vel[:] = quat_rotate_inverse(self.base_quat, self.root_states[:, 7:10])
   317        25    1245516.0  49820.6      0.0          self.base_ang_vel[:] = quat_rotate_inverse(self.base_quat, self.root_states[:, 10:13])
   318        25    1066513.0  42660.5      0.0          self.projected_gravity[:] = quat_rotate_inverse(self.base_quat, self.gravity_vec)
   319        25     955854.0  38234.2      0.0          self.base_lin_acc = (self.root_states[:, 7:10] - self.last_root_vel[:, :3]) / self.dt
   320                                           
   321        25     897554.0  35902.2      0.0          self.knee_pos = self.rigid_body_states.view(self.num_envs, self.num_bodies, 13)[:, self.knee_indices, 0:3]
   322        25     344439.0  13777.6      0.0          self.feet_pos = self.rigid_body_states.view(self.num_envs, self.num_bodies, 13)[:, self.feet_indices, 0:3]
   323                                           
   324        25    9137776.0 365511.0      0.1          self.roll, self.pitch, self.yaw = euler_from_quaternion(self.base_quat)
   325                                           
   326        25    1953975.0  78159.0      0.0          contact = torch.norm(self.contact_forces[:, self.feet_indices], dim=-1) > 2.
   327        25    9308080.0 372323.2      0.1          self.contact_filt = torch.logical_or(contact, self.last_contacts) 
   328        25      45057.0   1802.3      0.0          self.last_contacts = contact
   329                                                   
   330                                                   # self._update_jump_schedule()
   331        25   59175572.0 2.37e+06      0.9          self._update_goals()
   332        25  173833346.0 6.95e+06      2.6          self._post_physics_step_callback()
   333                                           
   334                                                   # compute observations, rewards, resets, ...
   335        25   26474755.0 1.06e+06      0.4          self.check_termination()
   336        25   43737468.0 1.75e+06      0.6          self.compute_reward()
   337        25    8673346.0 346933.8      0.1          env_ids = self.reset_buf.nonzero(as_tuple=False).flatten()
   338        25 6360853472.0 2.54e+08     94.4          self.reset_idx(env_ids)
   339                                           
   340        25    1287366.0  51494.6      0.0          self.cur_goals = self._gather_cur_goals()
   341        25     346575.0  13863.0      0.0          self.next_goals = self._gather_cur_goals(future=1)
   342                                           
   343                                                   # self.update_depth_buffer()
   344                                           
   345        25   10481481.0 419259.2      0.2          self.compute_observations() # in some cases a simulation step might be required to refresh some obs (for example body positions)
   346                                           
   347        25     188254.0   7530.2      0.0          self.last_last_actions[:] = self.last_actions[:]
   348        25     114308.0   4572.3      0.0          self.last_actions[:] = self.actions[:]
   349        25     167522.0   6700.9      0.0          self.last_dof_vel[:] = self.dof_vel[:]
   350        25     123517.0   4940.7      0.0          self.last_torques[:] = self.torques[:]
   351        25     145205.0   5808.2      0.0          self.last_root_vel[:] = self.root_states[:, 7:13]
   352        25      17298.0    691.9      0.0          if(self.time_stamp ==5):
   353         4      41114.0  10278.5      0.0              self.last_foot_action = self.rigid_body_states[:, self.feet_indices, :]
   354         4       2573.0    643.2      0.0              self.time_stamp=0
   355                                                   else :
   356        21      13633.0    649.2      0.0              self.time_stamp=self.time_stamp+1
   357                                                   
   358        25      18811.0    752.4      0.0          if self.viewer and self.enable_viewer_sync and self.debug_viz:
   359                                                       self.gym.clear_lines(self.viewer)
   360                                                       # self._draw_height_samples()
   361                                                       self._draw_goals()
   362                                                       # self._draw_feet()
   363                                                       if self.cfg.depth.use_camera:
   364                                                           window_name = "Depth Image"
   365                                                           cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
   366                                                           cv2.imshow("Depth Image", self.depth_buffer[self.lookat_id, -1].cpu().numpy() + 0.5)
   367                                                           cv2.waitKey(1)
   368                                           
   369        25     185473.0   7418.9      0.0          cur_knee_pos_trans = self.knee_pos - self.root_states[:, 0:3].unsqueeze(1)
   370        75     347964.0   4639.5      0.0          for i in range(len(self.knee_indices)):
   371        50    3554299.0  71086.0      0.1              self.knee_pos_in_body[:, i, :] = quat_rotate_inverse(self.base_quat, cur_knee_pos_trans[:, i, :])
   372                                           
   373        25     182072.0   7282.9      0.0          cur_feet_pos_trans = self.feet_pos - self.root_states[:, 0:3].unsqueeze(1)
   374        75      91343.0   1217.9      0.0          for i in range(len(self.feet_indices)):
   375        50    1291228.0  25824.6      0.0              self.feet_pos_in_body[:, i, :] = quat_rotate_inverse(self.base_quat, cur_feet_pos_trans[:, i, :])

Total time: 24.0841 s
File: /home/lxz/ICCV2025-Challenge/legged_gym/legged_gym/envs/base/humanoid_robot.py
Function: step at line 151

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   151                                               @profile
   152                                               def step(self, actions):
   153                                                   """ Apply actions, simulate, call self.post_physics_step()
   154                                           
   155                                                   Args:
   156                                                       actions (torch.Tensor): Tensor of shape (num_envs, num_actions_per_env)
   157                                                   """
   158        24      23867.0    994.5      0.0          start_t = time()
   159        24      96235.0   4009.8      0.0          actions.to(self.device)
   160        24     833480.0  34728.3      0.0          self.action_history_buf = torch.cat([self.action_history_buf[:, 1:].clone(), actions[:, None, :].clone()], dim=1)
   161        24      40272.0   1678.0      0.0          if self.cfg.domain_rand.action_delay:
   162        24      22697.0    945.7      0.0              if self.global_counter % self.cfg.domain_rand.delay_update_global_steps == 0:
   163         1        971.0    971.0      0.0                  if len(self.cfg.domain_rand.action_curr_step) != 0:
   164         1      28963.0  28963.0      0.0                      self.delay = torch.tensor(self.cfg.domain_rand.action_curr_step.pop(0), device=self.device, dtype=torch.float)
   165        24      15721.0    655.0      0.0              if self.viewer:
   166                                                           self.delay = torch.tensor(self.cfg.domain_rand.action_delay_view, device=self.device, dtype=torch.float)
   167        24     192454.0   8018.9      0.0              indices = -self.delay -1
   168        24    6092882.0 253870.1      0.0              actions = self.action_history_buf[:, indices.long()] # delay for 1/50=20ms
   169                                           
   170        24      14631.0    609.6      0.0          self.global_counter += 1
   171        24      12626.0    526.1      0.0          self.total_env_steps_counter += 1
   172        24      40663.0   1694.3      0.0          clip_actions = self.cfg.normalization.clip_actions / self.cfg.control.action_scale
   173        24     305520.0  12730.0      0.0          self.actions = torch.clip(actions, -clip_actions, clip_actions).to(self.device)
   174        24      74536.0   3105.7      0.0          self.render()
   175                                                   # print("1111 coumse: ", time()-start_t)
   176                                           
   177        24       9636.0    401.5      0.0          start_t = time()
   178       120     125476.0   1045.6      0.0          for _ in range(self.cfg.control.decimation):
   179        96   14715259.0 153283.9      0.1              self.torques = self._compute_torques(self.actions).view(self.torques.shape)
   180        96   52263938.0 544416.0      0.2              self.gym.set_dof_actuation_force_tensor(self.sim, gymtorch.unwrap_tensor(self.torques))
   181        96     1.72e+10 1.79e+08     71.5              self.gym.simulate(self.sim)
   182        96  180977908.0 1.89e+06      0.8              self.gym.fetch_results(self.sim, True)
   183        96   20405313.0 212555.3      0.1              self.gym.refresh_dof_state_tensor(self.sim)
   184        24 6597761391.0 2.75e+08     27.4          self.post_physics_step()
   185                                                   # print("2222 coumse: ", time()-start_t)
   186                                           
   187        24      22353.0    931.4      0.0          start_t = time()
   188        24      26494.0   1103.9      0.0          clip_obs = self.cfg.normalization.clip_observations
   189        24     164129.0   6838.7      0.0          self.obs_buf = torch.clip(self.obs_buf, -clip_obs, clip_obs)
   190        24      13953.0    581.4      0.0          if self.privileged_obs_buf is not None:
   191        24      91074.0   3794.8      0.0              self.privileged_obs_buf = torch.clip(self.privileged_obs_buf, -clip_obs, clip_obs)
   192        24     248705.0  10362.7      0.0          self.extras["delta_yaw_ok"] = self.delta_yaw < 0.6
   193        24      29908.0   1246.2      0.0          if self.cfg.depth.use_camera and self.global_counter % self.cfg.depth.update_interval == 0:
   194                                                       self.extras["depth"] = self.depth_buffer[:, -2]  # have already selected last one
   195                                                   else:
   196        24      10977.0    457.4      0.0              self.extras["depth"] = None
   197                                                   # print("3333 coumse: ", time()-start_t)
   198                                           
   199        24      17517.0    729.9      0.0          if self.save:
   200                                                       for env_idx in range(self.num_envs):
   201                                                           self.current_episode_buffer['observations'][env_idx].append(
   202                                                               self.obs_buf[env_idx].cpu().numpy().copy())  
   203                                                           self.current_episode_buffer['actions'][env_idx].append(
   204                                                               self.actions[env_idx].cpu().numpy().copy())      
   205                                                           
   206                                                           self.current_episode_buffer['rewards'][env_idx].append(
   207                                                               self.rew_buf[env_idx].cpu().numpy().copy()) 
   208                                                           
   209                                                           self.current_episode_buffer['height_map'][env_idx].append(
   210                                                               self.measured_heights_data[env_idx].cpu().numpy().copy()) 
   211                                                           
   212                                                           self.current_episode_buffer['rigid_body_state'][env_idx].append(
   213                                                               self.rigid_body_states[env_idx].cpu().numpy().copy()) 
   214                                                           
   215                                                           self.current_episode_buffer['dof_state'][env_idx].append(
   216                                                               self.dof_state[env_idx].cpu().numpy().copy())  
   217                                           
   218                                                           if self.privileged_obs_buf is not None:
   219                                                               self.current_episode_buffer['privileged_obs'][env_idx].append(
   220                                                                   self.privileged_obs_buf[env_idx].cpu().numpy().copy())      
   221                                           
   222        24      26046.0   1085.2      0.0          if(self.cfg.rewards.is_play):
   223                                                       if(self.total_times > 0):
   224                                                           if(self.total_times > self.last_times):
   225                                                               # print("total_times=",self.total_times)
   226                                                               # print("success_rate=",self.success_times / self.total_times)
   227                                                               # print("complete_rate=",(self.complete_times / self.total_times).cpu().numpy().copy())
   228                                                               self.last_times = self.total_times
   229                                           
   230        24      13400.0    558.3      0.0          return self.obs_buf, self.privileged_obs_buf, self.rew_buf, self.reset_buf, self.extras

  6.42 seconds - /home/lxz/ICCV2025-Challenge/legged_gym/legged_gym/envs/base/humanoid_robot.py:398 - reset_idx
  6.74 seconds - /home/lxz/ICCV2025-Challenge/legged_gym/legged_gym/envs/base/humanoid_robot.py:299 - post_physics_step
 24.08 seconds - /home/lxz/ICCV2025-Challenge/legged_gym/legged_gym/envs/base/humanoid_robot.py:151 - step
